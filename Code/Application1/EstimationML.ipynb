{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EntropyEstimation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPjCW3fQYUWFlXH7HP2d+iI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christianbentz/Workshop_DGfS2022/blob/main/Code/Application1/EstimationML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy Estimation with Maximum Likelihood Method\n",
        "\n",
        "Author: Chris Bentz\n",
        "\n",
        "Date: 18/02/2022"
      ],
      "metadata": {
        "id": "i9JNTF6_0eUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries\n",
        "Some packages are already pre-installed on jupyter, but some need to be installed. Run this code to make sure that the packages/libraries needed to run this code are installed."
      ],
      "metadata": {
        "id": "1C0c4iAr0JzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"stringr\")\n",
        "install.packages(\"entropy\")\n",
        "install.packages(\"quanteda\")"
      ],
      "metadata": {
        "id": "XSCPlmn30PxD",
        "outputId": "c5a37ff0-6f4c-4cdb-dcad-d42a3a6ad2d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘ISOcodes’, ‘fastmatch’, ‘RcppParallel’, ‘SnowballC’, ‘stopwords’, ‘RcppArmadillo’\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Libraries\n",
        "\n",
        "If the libraries are not installed yet, you need to install them using, for example, the command: install.packages(\"ggplot2\")."
      ],
      "metadata": {
        "id": "FIzBQUXEz769"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(stringr)\n",
        "library(entropy)\n",
        "library(quanteda)"
      ],
      "metadata": {
        "id": "Z-Pxl_Dnz8hv",
        "outputId": "f9d831b8-952f-4d63-954c-404d36ab6c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Package version: 3.2.0\n",
            "Unicode version: 10.0\n",
            "ICU version: 60.2\n",
            "\n",
            "Parallel computing: 2 of 2 threads used.\n",
            "\n",
            "See https://quanteda.io for tutorials and examples.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List Files\n",
        "Create list with all the file paths of files which are about to be processed further."
      ],
      "metadata": {
        "id": "qUEqwMkt0H6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filelist <- list.files(path = \"/content/processed\", \n",
        "                        recursive = T, full.names = T)\n",
        "head(filelist)\n",
        "length(filelist)"
      ],
      "metadata": {
        "id": "HldyGwmm0xaU",
        "outputId": "1b59b1a8-7d0e-4806-9767-55c69bc4d4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] \"/content/processed/unclassified_voy_0001.txt\"\n",
              "[2] \"/content/processed/writing_aii_0001.txt\"     \n",
              "[3] \"/content/processed/writing_arb_0001.txt\"     \n",
              "[4] \"/content/processed/writing_azj_0001.txt\"     \n",
              "[5] \"/content/processed/writing_azj_0002.txt\"     \n",
              "[6] \"/content/processed/writing_ben_0001.txt\"     "
            ],
            "text/latex": "\\begin{enumerate*}\n\\item '/content/processed/unclassified\\_voy\\_0001.txt'\n\\item '/content/processed/writing\\_aii\\_0001.txt'\n\\item '/content/processed/writing\\_arb\\_0001.txt'\n\\item '/content/processed/writing\\_azj\\_0001.txt'\n\\item '/content/processed/writing\\_azj\\_0002.txt'\n\\item '/content/processed/writing\\_ben\\_0001.txt'\n\\end{enumerate*}\n",
            "text/markdown": "1. '/content/processed/unclassified_voy_0001.txt'\n2. '/content/processed/writing_aii_0001.txt'\n3. '/content/processed/writing_arb_0001.txt'\n4. '/content/processed/writing_azj_0001.txt'\n5. '/content/processed/writing_azj_0002.txt'\n6. '/content/processed/writing_ben_0001.txt'\n\n\n",
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'/content/processed/unclassified_voy_0001.txt'</li><li>'/content/processed/writing_aii_0001.txt'</li><li>'/content/processed/writing_arb_0001.txt'</li><li>'/content/processed/writing_azj_0001.txt'</li><li>'/content/processed/writing_azj_0002.txt'</li><li>'/content/processed/writing_ben_0001.txt'</li></ol>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] 51"
            ],
            "text/latex": "51",
            "text/markdown": "51",
            "text/html": [
              "51"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character Entropy Estimation\n",
        "Estimate character entropy by using relative frequencies of characters in the text."
      ],
      "metadata": {
        "id": "Vmz_MfnM2IXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start time\n",
        "start_time <- Sys.time()\n",
        "\n",
        "# set counter\n",
        "counter = 0\n",
        "\n",
        "# initialize data frame to append results to\n",
        "entropy.df <- data.frame(filename = character(0), subcorpus = character(0), \n",
        "                         id = character(0), h.unigrams = numeric (0),\n",
        "                         h.bigrams = numeric(0), h.trigrams = numeric(0)) \n",
        "\n",
        "for (file in filelist) {\n",
        "  # loading textfile (\"skip\" specifies the number of lines to skip, whereas\n",
        "  # nmax gives the max number of lines to read.)\n",
        "  chars <- scan(file, what = \"char\", quote = \"\", comment.char = \"\", \n",
        "                   encoding = \"UTF-8\", sep = \" \" , skip = 0, nmax = F) \n",
        "  # get filename\n",
        "  filename <- basename(file) \n",
        "  # get subcorpus category\n",
        "  subcorpus <- sub(\"_.*\", \"\", filename)\n",
        "  # get three letter identifier from filename\n",
        "  id <- substr(str_extract(filename, \"_.*_\"), 2, 4) \n",
        "  \n",
        "  # estimate entropy for character unigrams\n",
        "  chars.uni.df <- as.data.frame(table(chars))\n",
        "  h.unigrams <- entropy(chars.uni.df$Freq, method = \"ML\", unit = \"log2\")\n",
        "  \n",
        "  # estimate entropy for character bigrams\n",
        "  chars.bi <- char_ngrams(chars, n = 2, concatenator = \"\")\n",
        "  chars.bi.df <- as.data.frame(table(chars.bi)) \n",
        "  h.bigrams <- entropy(chars.bi.df$Freq, method = \"ML\", unit = \"log2\")\n",
        "  \n",
        "  # estimate entropy for character trigrams\n",
        "  chars.tri <- char_ngrams(chars, n = 3, concatenator = \"\")\n",
        "  chars.tri.df <- as.data.frame(table(chars.tri)) \n",
        "  h.trigrams <- entropy(chars.tri.df$Freq, method = \"ML\", unit = \"log2\")\n",
        "  \n",
        "  # append results to data frame\n",
        "  local.df <- data.frame(filename, subcorpus, id, h.unigrams, \n",
        "                        h.bigrams, h.trigrams)\n",
        "  entropy.df <- rbind(entropy.df, local.df)\n",
        "  # counter\n",
        "  counter <- counter + 1\n",
        "  # print(counter)\n",
        "}\n",
        "\n",
        "# get end time\n",
        "end_time <- Sys.time()\n",
        "end_time - start_time\n",
        "\n",
        "# show final data frame with results\n",
        "print(entropy.df)"
      ],
      "metadata": {
        "id": "GuSswh2s2OIf",
        "outputId": "e40b0f5f-3be8-4f62-8082-c974c1e39796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Time difference of 1.729106 secs"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    filename    subcorpus  id h.unigrams h.bigrams h.trigrams\n",
            "1  unclassified_voy_0001.txt unclassified voy   3.850406  6.182006   8.103153\n",
            "2       writing_aii_0001.txt      writing aii   4.172559  7.613666   9.779882\n",
            "3       writing_arb_0001.txt      writing arb   4.548830  8.241311  10.512547\n",
            "4       writing_azj_0001.txt      writing azj   4.613046  8.077250  10.258100\n",
            "5       writing_azj_0002.txt      writing azj   4.611793  8.076691  10.257565\n",
            "6       writing_ben_0001.txt      writing ben   4.955011  8.471218  10.427342\n",
            "7       writing_blt_0001.txt      writing blt   5.300575  8.838212  10.563588\n",
            "8       writing_bod_0001.txt      writing bod   4.456135  7.410592   9.402117\n",
            "9       writing_bos_0001.txt      writing bos   4.603988  7.966511  10.310123\n",
            "10      writing_bos_0002.txt      writing bos   4.532071  7.854217  10.212521\n",
            "11      writing_chr_0001.txt      writing chr   5.489144  8.920484  10.558936\n",
            "12      writing_cmn_0001.txt      writing cmn   7.690635 10.161392  10.922859\n",
            "13      writing_cmn_0002.txt      writing cmn   7.649083 10.081220  10.832478\n",
            "14      writing_csw_0001.txt      writing csw   5.118661  8.563493  10.088003\n",
            "15      writing_div_0001.txt      writing div   4.559061  7.526136   9.617775\n",
            "16      writing_ell_0001.txt      writing ell   5.308691  8.816208  10.825349\n",
            "17      writing_eng_0001.txt      writing eng   4.359670  7.767830  10.064556\n",
            "18      writing_epo_0001.txt      writing epo   4.403043  7.753435  10.094334\n",
            "19      writing_eus_0001.txt      writing eus   4.134548  7.379460   9.717702\n",
            "20      writing_gaz_0001.txt      writing gaz   4.230948  7.343073   9.513023\n",
            "21      writing_guj_0001.txt      writing guj   4.961668  8.446244  10.317615\n",
            "22      writing_heb_0001.txt      writing heb   4.401510  8.151383  10.570399\n",
            "23      writing_hin_0001.txt      writing hin   5.002764  8.524404  10.501771\n",
            "24      writing_hye_0001.txt      writing hye   4.638029  7.813005   9.853440\n",
            "25      writing_ibb_0001.txt      writing ibb   4.051891  7.123654   9.299677\n",
            "26      writing_iii_0001.txt      writing iii   6.934642  9.686964  10.707230\n",
            "27      writing_ike_0001.txt      writing ike   5.521849  8.921545  10.642025\n",
            "28      writing_jav_0001.txt      writing jav   4.741243  8.119481  10.277767\n",
            "29      writing_jav_0002.txt      writing jav   4.241845  7.365354   9.783904\n",
            "30      writing_jpn_0001.txt      writing jpn   7.149899  9.676028  10.660800\n",
            "31      writing_kal_0001.txt      writing kal   3.830474  6.565880   8.687897\n",
            "32      writing_kan_0001.txt      writing kan   4.983140  8.583629  10.459907\n",
            "33      writing_kat_0001.txt      writing kat   4.370298  7.714711   9.994755\n",
            "34      writing_khm_0001.txt      writing khm   5.128601  8.777600  10.533473\n",
            "35      writing_kkh_0001.txt      writing kkh   5.064581  8.359711  10.211974\n",
            "36      writing_kor_0001.txt      writing kor   7.128560  9.816783  10.659020\n",
            "37      writing_lao_0001.txt      writing lao   5.265673  8.770887  10.593355\n",
            "38      writing_lug_0001.txt      writing lug   4.332187  7.354721   9.605968\n",
            "39      writing_mal_0001.txt      writing mal   4.808532  8.069228   9.943784\n",
            "40      writing_mya_0001.txt      writing mya   4.910854  7.941910   9.840716\n",
            "41      writing_pan_0001.txt      writing pan   5.146618  8.718630  10.635902\n",
            "42      writing_rus_0001.txt      writing rus   4.658441  8.147920  10.380960\n",
            "43      writing_sin_0001.txt      writing sin   4.973843  8.622505  10.586959\n",
            "44      writing_tam_0001.txt      writing tam   4.497372  7.761642   9.726090\n",
            "45      writing_tel_0001.txt      writing tel   4.818831  8.230364  10.202741\n",
            "46      writing_tgl_0001.txt      writing tgl   3.784131  6.618254   8.816390\n",
            "47      writing_tha_0001.txt      writing tha   5.328191  8.944341  10.583950\n",
            "48      writing_tir_0001.txt      writing tir   6.043467  9.588858  10.850239\n",
            "49      writing_vai_0001.txt      writing vai   5.911782  9.273204  10.930672\n",
            "50      writing_zgh_0001.txt      writing zgh   4.184790  7.612214  10.001843\n",
            "51      writing_zul_0001.txt      writing zul   4.317443  7.327384   9.592265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write Table to File\n",
        "Write the table as comma separted values (csv) to a file (useful for checking the table in a regular program like excel, and later loading it into another session)."
      ],
      "metadata": {
        "id": "y5X3Nkj23L05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "write.csv(entropy.df, \"/content/results/entropy_ML.csv\", row.names = F)"
      ],
      "metadata": {
        "id": "Hg-20szs3FQf"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}