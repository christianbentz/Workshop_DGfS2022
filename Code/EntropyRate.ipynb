{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_EntropyRate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Entropy rate using a neural language model"
      ],
      "metadata": {
        "id": "AXzhbykylOgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The libraries we are going to use:"
      ],
      "metadata": {
        "id": "sOsfmf6kCfo7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FuPM5zVMCIDG"
      },
      "outputs": [],
      "source": [
        "import torch         #For Neural Networks\n",
        "import torch.nn as nn  #For Neural Networks\n",
        "import numpy as np     \n",
        "from nltk import ngrams   #Extracting ngrams from any string\n",
        "import os #Reading file directories and other options\n",
        "from tqdm import tqdm #Progress bar\n",
        "import csv  #for writing output in csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define functions that we are going to need later:"
      ],
      "metadata": {
        "id": "LNlVYn7rCftg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to obtain character ngrams from a text:\n",
        "def get_char_ngrams(text, n):\n",
        "  ngrams_list=[]\n",
        " \n",
        "  for ngram in ngrams(text, n):\n",
        "      ngram_string=\"\".join(ngram)\n",
        "      ngrams_list.append(ngram_string)\n",
        "  return(ngrams_list)  #Returns a list of the char ngrams contained in the input text\n",
        "\n",
        "#Function to convert the input text (already converted to char ngrams) to index numbers:\n",
        "def text2number(char_ngrams):\n",
        "  index=0\n",
        "  indexed_line=[]\n",
        "  vocabulary={}\n",
        "  for t in char_ngrams: #for each ngram in the list\n",
        "    if not(t in vocabulary): #If we didn't indexed it already\n",
        "      vocabulary[t]=index\n",
        "      #print(w, index)\n",
        "      index=index+1\n",
        "         \n",
        "    indexed_line.append(vocabulary[t])\n",
        "\n",
        "  return (vocabulary, indexed_line)  #Returns a dictionary of the index values, and the text (already converted to char ngrams) converted to numeric indexes "
      ],
      "metadata": {
        "id": "MH06ZNsqCwoc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Toy example**\n",
        "First let see how to preprocess the text in order to prepare the input for the neural network."
      ],
      "metadata": {
        "id": "ijCmHKyE56c_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First we get char trigrams:"
      ],
      "metadata": {
        "id": "uOlq3B4B81Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toycorpus = 'G I Z A E S K U B I D E E N A L D A R R I K A P E N U N I B E R T S A L A H I T Z A U R R E A K o n t u a n i z a n i k m u n'\n",
        "\n",
        "text=toycorpus.lower().replace(\" \", \"\")  #We lowercase and remove spaces\n",
        "char_trigrams=get_char_ngrams(text,3)   #We obtain trigrams of characters\n",
        "print(char_trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJLvanuy6aVn",
        "outputId": "0dad7b4c-c3a7-4dc1-9267-2630f0f021fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['giz', 'iza', 'zae', 'aes', 'esk', 'sku', 'kub', 'ubi', 'bid', 'ide', 'dee', 'een', 'ena', 'nal', 'ald', 'lda', 'dar', 'arr', 'rri', 'rik', 'ika', 'kap', 'ape', 'pen', 'enu', 'nun', 'uni', 'nib', 'ibe', 'ber', 'ert', 'rts', 'tsa', 'sal', 'ala', 'lah', 'ahi', 'hit', 'itz', 'tza', 'zau', 'aur', 'urr', 'rre', 'rea', 'eak', 'ako', 'kon', 'ont', 'ntu', 'tua', 'uan', 'ani', 'niz', 'iza', 'zan', 'ani', 'nik', 'ikm', 'kmu', 'mun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. We represent the text (already converted to trigrams) as numeric indexes. This way the neural network will be able to process it."
      ],
      "metadata": {
        "id": "w0HhUnzO86VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out=text2number(char_trigrams) #it returns 2 objects, the dictionary with indexes, and the converted text\n",
        "idx=out[0]  #the dictionary with indexes\n",
        "indexed_corpus=out[1] #the converted text\n",
        "print(idx)\n",
        "print(indexed_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9M3aAPo8y3v",
        "outputId": "42697694-a3bf-424b-bb8e-63a8436810e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'giz': 0, 'iza': 1, 'zae': 2, 'aes': 3, 'esk': 4, 'sku': 5, 'kub': 6, 'ubi': 7, 'bid': 8, 'ide': 9, 'dee': 10, 'een': 11, 'ena': 12, 'nal': 13, 'ald': 14, 'lda': 15, 'dar': 16, 'arr': 17, 'rri': 18, 'rik': 19, 'ika': 20, 'kap': 21, 'ape': 22, 'pen': 23, 'enu': 24, 'nun': 25, 'uni': 26, 'nib': 27, 'ibe': 28, 'ber': 29, 'ert': 30, 'rts': 31, 'tsa': 32, 'sal': 33, 'ala': 34, 'lah': 35, 'ahi': 36, 'hit': 37, 'itz': 38, 'tza': 39, 'zau': 40, 'aur': 41, 'urr': 42, 'rre': 43, 'rea': 44, 'eak': 45, 'ako': 46, 'kon': 47, 'ont': 48, 'ntu': 49, 'tua': 50, 'uan': 51, 'ani': 52, 'niz': 53, 'zan': 54, 'nik': 55, 'ikm': 56, 'kmu': 57, 'mun': 58}\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 1, 54, 52, 55, 56, 57, 58]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Once we have lists of numeric indexes representing the char ngrams in words, we'll just format them as training pairs (x,y) for the neural network. \n",
        "\n",
        "These training pairs are used by the neural network to model p(wj|wi), i.e., the probability of a char trigram given another char trigram in that language. "
      ],
      "metadata": {
        "id": "b5au_QBH-CYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_bigrams = list(ngrams(indexed_corpus, 2))  #We extract bigrams of char trigrams to form training pairs: (x,y)\n",
        "print(\"Training_bigrams per word:\")\n",
        "print(training_bigrams)\n",
        "\n",
        "print(\"We make two lists, one with all x values, another one with Y values. These will enter to the NN:\")\n",
        "print(list(zip(*training_bigrams)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSLNOUt0BKh4",
        "outputId": "7922d120-f04a-4407-e33b-dba051674df1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_bigrams per word:\n",
            "[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (44, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 1), (1, 54), (54, 52), (52, 55), (55, 56), (56, 57), (57, 58)]\n",
            "We make two lists, one with all x values, another one with Y values. These will enter to the NN:\n",
            "[(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 1, 54, 52, 55, 56, 57), (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 1, 54, 52, 55, 56, 57, 58)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Training the Neural Network**"
      ],
      "metadata": {
        "id": "MfIQc7-lNqhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We split the training pairs in two different lists (x and y), and we convert them to tensor data format. This is required by Pytorch:\n",
        "x,y = torch.tensor(list(zip(*training_bigrams)), dtype=torch.long)\n",
        "print(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK0VmUcpC65P",
        "outputId": "e882e75f-4f6f-4777-d313-06455c681680"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "         1, 54, 52, 55, 56, 57]) tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
            "        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
            "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,  1,\n",
            "        54, 52, 55, 56, 57, 58])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Neural Netowrk Parameters\n",
        "dim_in = len(idx)  #The dimension of the input layer (the size of the vocabulary)\n",
        "dim = 300     #The dimension of the embedding layer\n",
        "dim_h = 100  #The dimension of the hidden layer\n",
        "dim_out = len(idx)  #The dimension of the output layer (the size of the vocabulary)\n",
        "\n",
        "#We define the architecture of the Neural Network (feed forward) layer by layer:\n",
        "#The final layer (output) is a softmax from which we will retrieve the probabilities we are looking for (once is trained):\n",
        "forward = nn.Sequential(nn.Embedding(dim_in,dim), nn.Linear(dim,dim_h), nn.Tanh(), nn.Linear(dim_h,dim_out), nn.Softmax(1)) \n",
        "\n",
        "\n",
        "#We define a loss function for the training:\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#Choosing the optimization algorithm (Stochastic gradient descent) and the learning rate\n",
        "optimizer = torch.optim.SGD(forward.parameters(), lr=0.1)\n",
        "\n",
        "#Number of iterations\n",
        "its = 10\n",
        "\n",
        "#We train the model\n",
        "for epoch in tqdm(range(its)):\n",
        "  for x,y in training_bigrams:\n",
        "      #FORWARD\n",
        "    x_in=torch.tensor([x], dtype=torch.long)\n",
        "    y_pred = forward(x_in)\n",
        "\n",
        "     #BACKWARD\n",
        "    #Error calculation\n",
        "    y_targ=torch.tensor([y], dtype=torch.long)\n",
        "    loss = criterion(target=y_targ, input=y_pred)\n",
        "    #zero grad\n",
        "    optimizer.zero_grad()\n",
        "    #Backprop\n",
        "    loss.backward()\n",
        "    #We update the values\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "2IwOoB6PEZNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488da265-eaae-4091-cb1e-8c3677ac32ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 30.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once is trained we extract the values from the output layer and we construct our stochastic matrix:"
      ],
      "metadata": {
        "id": "-3-OOyCyjitz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_index_vocabulary = torch.tensor(list(idx.values()))\n",
        "stochastic_matrix = forward(total_index_vocabulary).detach()\n",
        "\n",
        "\n",
        "mu = (1/len(idx))*stochastic_matrix.sum(0)\n",
        "entropy_rate = -(mu*(stochastic_matrix*np.log(stochastic_matrix)).sum(1)).sum(0)/np.log(len(idx))\n",
        " \n",
        "print('Entropy rate:', entropy_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjC3fXwKjeOF",
        "outputId": "58842f60-d0dc-4b7e-f2a6-e50da9826697"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy rate: tensor(0.5690)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training on the corpus"
      ],
      "metadata": {
        "id": "xDL-XlmxlWip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory=\"/content/processed/\" #Data source (change it to your Data location)\n",
        "all_files=[f for f in os.listdir(directory) if not f.startswith('.')] #listing all files in a directory (hidden files excluded)\n",
        "\n",
        "f = open('/content/entropyRate(trigrams).csv', 'w') #Output CSV (change it to your Data location)\n",
        "writer = csv.writer(f)  # create the csv writer\n",
        "csvheader = ['filename', 'Hrate.trigrams']  #The header of your file\n",
        "writer.writerow(csvheader)\n",
        "size_char_ngrams=3\n",
        "\n",
        "################################################################################\n",
        "\n",
        "for n in all_files:\n",
        "  print('Corpus:', n)\n",
        "  inputcorpus=directory+n\n",
        "  file=open(inputcorpus,'r', encoding=\"utf-8\")\n",
        "  text=file.read().lower().replace(\" \", \"\")  #We lowercase and remove spaces\n",
        "  char_ngrams=get_char_ngrams(text,size_char_ngrams)\n",
        "\n",
        "  out=text2number(char_ngrams) #it returns 2 objects, the dictionary with indexes, and the converted text\n",
        "  idx=out[0]  #the dictionary with indexes\n",
        "  indexed_corpus=out[1] #the converted text\n",
        "\n",
        "  training_bigrams = list(ngrams(indexed_corpus, 2))\n",
        "\n",
        "  \n",
        "  x,y = torch.tensor(list(zip(*training_bigrams)), dtype=torch.long)\n",
        "\n",
        "  #Neural Network Specifications\n",
        "  dim_in = len(idx)  #The dimension of the input layer (the size of the vocabulary)\n",
        "  dim = 300     #The dimension of the embedding layer\n",
        "  dim_h = 100  #The dimension of the hidden layer\n",
        "  dim_out = len(idx)  #The dimension of the output layer (the size of the vocabulary)\n",
        "\n",
        "  #We define the architecture of the Neural Network (feed forward) layer by layer:\n",
        "  #The final layer (output) is a softmax from which we will retrieve the probabilities we are looking for (once is trained):\n",
        "  forward = nn.Sequential(nn.Embedding(dim_in,dim), nn.Linear(dim,dim_h), nn.Tanh(), nn.Linear(dim_h,dim_out), nn.Softmax(1)) \n",
        "\n",
        "\n",
        "  #We define a loss function for the training:\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  #Choosing the optimization algorithm (Stochastic gradient descent) and the learning rate\n",
        "  optimizer = torch.optim.SGD(forward.parameters(), lr=0.1)\n",
        "\n",
        "  #Number of iterations\n",
        "  its = 35\n",
        "\n",
        "  #We train the model\n",
        "  for epoch in tqdm(range(its)):\n",
        "    for x,y in training_bigrams:\n",
        "        #FORWARD\n",
        "      x_in=torch.tensor([x], dtype=torch.long)\n",
        "      y_pred = forward(x_in)\n",
        "\n",
        "      #BACKWARD\n",
        "      #Error calculation\n",
        "      y_targ=torch.tensor([y], dtype=torch.long)\n",
        "      loss = criterion(target=y_targ, input=y_pred)\n",
        "      #zero grad\n",
        "      optimizer.zero_grad()\n",
        "      #Backprop\n",
        "      loss.backward()\n",
        "      #We update the values\n",
        "      optimizer.step()\n",
        "\n",
        "  total_index_vocabulary = torch.tensor(list(idx.values()))\n",
        "  stochastic_matrix = forward(total_index_vocabulary).detach()\n",
        "\n",
        "\n",
        "  mu = (1/len(idx))*stochastic_matrix.sum(0)\n",
        "  entropy_rate = -(mu*(stochastic_matrix*np.log(stochastic_matrix)).sum(1)).sum(0)/np.log(len(idx))\n",
        "\n",
        "  \n",
        "  print('Entropy rate:', entropy_rate)\n",
        "\n",
        "  csvrow = [n, float(entropy_rate)]\n",
        "  writer.writerow(csvrow)\n",
        " \n",
        "  \n",
        "f.close() # close the CSV file"
      ],
      "metadata": {
        "id": "HwQg0UkMlb6B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}