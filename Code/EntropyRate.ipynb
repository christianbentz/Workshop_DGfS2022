{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EntropyRate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The libraries we are going to use:"
      ],
      "metadata": {
        "id": "sOsfmf6kCfo7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "FuPM5zVMCIDG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import chain\n",
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define functions that we are going to need later:"
      ],
      "metadata": {
        "id": "LNlVYn7rCftg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions to obtain char ngrams from a text:\n",
        "def getngrams(text, n):\n",
        "  ngrams_list=[]\n",
        "  for w in text:\n",
        "    aux=[]\n",
        "    w=\"#\"+w+\"$\"\n",
        "    for ngram in ngrams(w, n):\n",
        "      ngram_string=\"\".join(ngram)\n",
        "      aux.append(ngram_string)\n",
        "    ngrams_list.append(aux)\n",
        "  return(ngrams_list)  #Returns a list of char ngrams for each word\n",
        "\n",
        "#Function to convert the input text to index numbers:\n",
        "def text2number(text):\n",
        "  index=0\n",
        "  indexed_line=[]\n",
        "  vocabulary={}\n",
        "  for word in text:\n",
        "    aux=[]\n",
        "    for t in word: #for each ngram in word\n",
        "      if not(t in vocabulary):\n",
        "        vocabulary[t]=index\n",
        "        #print(w, index)\n",
        "        index=index+1\n",
        "      aux.append(vocabulary[t])\n",
        "        \n",
        "    indexed_line.append(aux)\n",
        "\n",
        "  return (vocabulary, indexed_line)  #Returns a dictionary of the index values, and the text converted to numeric indexes "
      ],
      "metadata": {
        "id": "MH06ZNsqCwoc"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Toy example**\n",
        "First let see how to preprocess the text in order to prepare the input for the neural network."
      ],
      "metadata": {
        "id": "ijCmHKyE56c_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First we get char trigrams:"
      ],
      "metadata": {
        "id": "uOlq3B4B81Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toycorpus = 'The dog eats A boy eats The dog eats food'\n",
        "\n",
        "text=toycorpus.split()\n",
        "char_trigrams=getngrams(text,3)\n",
        "print(char_trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJLvanuy6aVn",
        "outputId": "653981d2-56a2-4fa4-f380-faf2536d0d7b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['#Th', 'The', 'he$'], ['#do', 'dog', 'og$'], ['#ea', 'eat', 'ats', 'ts$'], ['#A$'], ['#bo', 'boy', 'oy$'], ['#ea', 'eat', 'ats', 'ts$'], ['#Th', 'The', 'he$'], ['#do', 'dog', 'og$'], ['#ea', 'eat', 'ats', 'ts$'], ['#fo', 'foo', 'ood', 'od$']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. We represent the text (already converted to trigrams) as numeric indexes. This way the neural network will be able to process it."
      ],
      "metadata": {
        "id": "w0HhUnzO86VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out=text2number(char_trigrams) #it returns 2 objects, the dictionary with indexes, and the converted text\n",
        "idx=out[0]  #the dictionary with indexes\n",
        "indexed_corpus=out[1] #the converted text\n",
        "print(idx)\n",
        "print(indexed_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9M3aAPo8y3v",
        "outputId": "c4346ea5-4457-457b-f548-a47fb66bab86"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'#Th': 0, 'The': 1, 'he$': 2, '#do': 3, 'dog': 4, 'og$': 5, '#ea': 6, 'eat': 7, 'ats': 8, 'ts$': 9, '#A$': 10, '#bo': 11, 'boy': 12, 'oy$': 13, '#fo': 14, 'foo': 15, 'ood': 16, 'od$': 17}\n",
            "[[0, 1, 2], [3, 4, 5], [6, 7, 8, 9], [10], [11, 12, 13], [6, 7, 8, 9], [0, 1, 2], [3, 4, 5], [6, 7, 8, 9], [14, 15, 16, 17]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Once we have lists of numeric indexes representing the char ngrams in words, we'll just format them as training pairs (x,y) for the neural network  "
      ],
      "metadata": {
        "id": "b5au_QBH-CYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_bigrams = [list(ngrams(word, 2)) for word in indexed_corpus]\n",
        "print(\"Training_bigrams per word:\")\n",
        "print(training_bigrams)\n",
        "flat_list = []\n",
        "for sublist in training_bigrams:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)\n",
        "print(\"Training_bigrams in a single list:\")\n",
        "print(flat_list)\n",
        "#list(zip(*bigrams))\n",
        "print(\"We make two lists, one with all x values, another one with Y values. These enter to the NN:\")\n",
        "print(list(zip(*flat_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmcKwZ5D906T",
        "outputId": "7b6da8a4-89ab-4425-90be-0620588abf07"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training_bigrams per word:\n",
            "[[(0, 1), (1, 2)], [(3, 4), (4, 5)], [(6, 7), (7, 8), (8, 9)], [], [(11, 12), (12, 13)], [(6, 7), (7, 8), (8, 9)], [(0, 1), (1, 2)], [(3, 4), (4, 5)], [(6, 7), (7, 8), (8, 9)], [(14, 15), (15, 16), (16, 17)]]\n",
            "Training_bigrams in a single list:\n",
            "[(0, 1), (1, 2), (3, 4), (4, 5), (6, 7), (7, 8), (8, 9), (11, 12), (12, 13), (6, 7), (7, 8), (8, 9), (0, 1), (1, 2), (3, 4), (4, 5), (6, 7), (7, 8), (8, 9), (14, 15), (15, 16), (16, 17)]\n",
            "We make two lists, one with all x values, another one with Y values. These enter to the NN:\n",
            "[(0, 1, 3, 4, 6, 7, 8, 11, 12, 6, 7, 8, 0, 1, 3, 4, 6, 7, 8, 14, 15, 16), (1, 2, 4, 5, 7, 8, 9, 12, 13, 7, 8, 9, 1, 2, 4, 5, 7, 8, 9, 15, 16, 17)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Se pasan los datos a formato torch.tensor\n",
        "x,y = torch.tensor(list(zip(*flat_list)), dtype=torch.long)\n",
        "\n",
        "print(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK0VmUcpC65P",
        "outputId": "f7814674-15a7-438b-ddc0-ab281c10cf92"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  1,  3,  4,  6,  7,  8, 11, 12,  6,  7,  8,  0,  1,  3,  4,  6,  7,\n",
            "         8, 14, 15, 16]) tensor([ 1,  2,  4,  5,  7,  8,  9, 12, 13,  7,  8,  9,  1,  2,  4,  5,  7,  8,\n",
            "         9, 15, 16, 17])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Se definen los parámetros\n",
        "dim_in = len(idx)\n",
        "dim = 300\n",
        "dim_h = 100\n",
        "dim_out = len(idx)\n",
        "\n",
        "#Se define la arquitectura de la red (forward)\n",
        "forward = nn.Sequential(nn.Embedding(dim_in,dim), nn.Linear(dim,dim_h), nn.Tanh(), nn.Linear(dim_h,dim_out), nn.Softmax(1))\n",
        "\n",
        "#Se define la función de Riesgo (Entropía cruzada)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#Se define la forma de optimizar (Gradiente Descendiente Estocástico)\n",
        "optimizer = torch.optim.SGD(forward.parameters(), lr=0.1)\n",
        "\n",
        "#Numero de iteraciones\n",
        "its = 100\n",
        "\n",
        "#Se entrena el modelo\n",
        "for epoch in range(its):\n",
        "\t#FORWARD\n",
        "\ty_pred = forward(x)\n",
        "\n",
        "\t#BACKWARD\n",
        "\t#Se calcula el eror\n",
        "\tloss = criterion(y_pred, y)\n",
        "\t#zero grad\n",
        "\toptimizer.zero_grad()\n",
        "\t#Backprop\n",
        "\tloss.backward()\n",
        "\t#Se actualizan los parametros\n",
        "\toptimizer.step()"
      ],
      "metadata": {
        "id": "2IwOoB6PEZNn"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward(torch.tensor([4, 3, 5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XglG361pFLAC",
        "outputId": "0827fba7-c509-4bd9-970d-63de2cd43bc4"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.8087e-03, 1.8139e-03, 1.5639e-03, 1.4914e-03, 1.0422e-03, 9.7354e-01,\n",
              "         2.3855e-03, 1.1801e-03, 1.1771e-03, 9.4962e-04, 2.3034e-03, 2.2108e-03,\n",
              "         8.4550e-04, 2.2193e-03, 6.4067e-04, 2.2809e-03, 1.7790e-03, 7.6870e-04],\n",
              "        [1.3503e-03, 1.1519e-03, 9.3013e-04, 2.4567e-03, 9.7826e-01, 1.0793e-03,\n",
              "         8.9538e-04, 6.7726e-04, 2.6520e-04, 9.9960e-04, 1.5444e-03, 1.4471e-03,\n",
              "         1.2683e-03, 1.6494e-03, 1.5218e-03, 1.0875e-03, 1.3320e-03, 2.0810e-03],\n",
              "        [7.1858e-02, 1.3226e-01, 5.0552e-02, 4.5914e-02, 6.8726e-02, 1.1550e-01,\n",
              "         2.7136e-02, 6.3509e-02, 3.5941e-02, 1.7463e-02, 3.3605e-02, 6.5861e-02,\n",
              "         5.6442e-02, 3.7120e-02, 4.4536e-02, 4.2710e-02, 4.1959e-02, 4.8914e-02]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_index_vocabulary = torch.tensor(list(out[0].values()))\n",
        "print(total_index_vocabulary)\n",
        "stochastic_matrix = forward(total_index_vocabulary).detach()\n",
        "print(stochastic_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOIEcWwZHZqy",
        "outputId": "8dbc71d0-2057-4c24-c2fa-f86ebedb30d5"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
            "tensor([[1.8071e-03, 9.7669e-01, 2.1356e-04, 1.2520e-03, 1.1186e-03, 1.6606e-03,\n",
            "         1.1259e-03, 2.4686e-03, 8.8755e-04, 1.2034e-03, 1.5772e-03, 2.0886e-03,\n",
            "         1.7688e-03, 5.3331e-04, 9.6659e-04, 1.7569e-03, 1.7161e-03, 1.1700e-03],\n",
            "        [1.3232e-03, 3.5071e-04, 9.7381e-01, 1.8180e-03, 1.1995e-03, 1.8354e-03,\n",
            "         1.2123e-03, 1.0623e-03, 1.5120e-03, 3.9787e-03, 1.3511e-03, 1.3930e-03,\n",
            "         1.7087e-03, 2.4099e-03, 9.0610e-04, 1.9463e-03, 5.9042e-04, 1.5969e-03],\n",
            "        [6.1099e-02, 4.3456e-02, 9.1318e-02, 4.0740e-02, 4.9374e-02, 2.4417e-02,\n",
            "         2.6835e-02, 5.8201e-02, 2.2128e-02, 4.2882e-02, 3.1887e-02, 4.5851e-02,\n",
            "         1.7363e-01, 6.0953e-02, 4.8396e-02, 9.6602e-02, 4.9141e-02, 3.3093e-02],\n",
            "        [1.3503e-03, 1.1519e-03, 9.3013e-04, 2.4567e-03, 9.7826e-01, 1.0793e-03,\n",
            "         8.9538e-04, 6.7726e-04, 2.6520e-04, 9.9960e-04, 1.5444e-03, 1.4471e-03,\n",
            "         1.2683e-03, 1.6493e-03, 1.5218e-03, 1.0875e-03, 1.3320e-03, 2.0810e-03],\n",
            "        [1.8087e-03, 1.8139e-03, 1.5639e-03, 1.4914e-03, 1.0422e-03, 9.7354e-01,\n",
            "         2.3855e-03, 1.1801e-03, 1.1771e-03, 9.4962e-04, 2.3034e-03, 2.2108e-03,\n",
            "         8.4550e-04, 2.2193e-03, 6.4067e-04, 2.2809e-03, 1.7790e-03, 7.6870e-04],\n",
            "        [7.1858e-02, 1.3226e-01, 5.0552e-02, 4.5914e-02, 6.8726e-02, 1.1550e-01,\n",
            "         2.7136e-02, 6.3509e-02, 3.5941e-02, 1.7463e-02, 3.3605e-02, 6.5861e-02,\n",
            "         5.6442e-02, 3.7120e-02, 4.4536e-02, 4.2710e-02, 4.1959e-02, 4.8914e-02],\n",
            "        [8.5248e-04, 1.8828e-03, 3.4237e-04, 6.7721e-04, 4.7779e-04, 3.9247e-04,\n",
            "         5.5332e-04, 9.8527e-01, 8.0896e-04, 5.6318e-04, 1.2632e-03, 3.4683e-04,\n",
            "         9.0611e-04, 9.3460e-04, 9.9339e-04, 1.6596e-03, 1.3433e-03, 7.2881e-04],\n",
            "        [5.4323e-04, 4.9976e-04, 9.6421e-04, 9.9930e-04, 1.2220e-04, 4.8167e-04,\n",
            "         9.0013e-04, 1.0566e-03, 9.8777e-01, 4.0763e-04, 6.5261e-04, 8.5961e-04,\n",
            "         5.5861e-04, 2.7806e-04, 6.9134e-04, 1.3039e-03, 8.7750e-04, 1.0291e-03],\n",
            "        [8.4584e-04, 6.2504e-04, 1.8635e-03, 4.5106e-04, 6.4130e-04, 4.9670e-04,\n",
            "         1.2332e-03, 5.0657e-04, 6.3098e-04, 9.8612e-01, 7.5419e-04, 9.3967e-04,\n",
            "         1.5066e-03, 5.6380e-04, 9.8608e-04, 4.7286e-04, 3.7227e-04, 9.9515e-04],\n",
            "        [4.1384e-02, 6.2891e-02, 5.5008e-02, 6.1123e-02, 6.9344e-02, 8.3935e-02,\n",
            "         6.9320e-02, 4.2152e-02, 6.4526e-02, 5.2590e-02, 4.5369e-02, 4.4441e-02,\n",
            "         3.6455e-02, 1.0287e-01, 4.6399e-02, 3.5626e-02, 5.5732e-02, 3.0840e-02],\n",
            "        [6.1608e-02, 9.6038e-02, 4.9208e-02, 2.9993e-02, 1.0317e-01, 2.2391e-02,\n",
            "         2.7158e-02, 1.5984e-02, 2.7070e-02, 1.0700e-01, 4.1927e-02, 5.7508e-02,\n",
            "         3.0550e-02, 2.9381e-02, 4.6393e-02, 8.5988e-02, 4.9080e-02, 1.1956e-01],\n",
            "        [5.3511e-03, 9.5034e-03, 6.7197e-03, 4.5648e-03, 7.0323e-03, 2.9007e-03,\n",
            "         7.7170e-03, 5.5805e-03, 3.1442e-03, 9.6903e-03, 4.1583e-03, 6.6520e-03,\n",
            "         9.0629e-01, 3.2729e-03, 6.9634e-03, 4.3377e-03, 3.8773e-03, 2.2421e-03],\n",
            "        [5.6226e-03, 1.6755e-03, 5.1696e-03, 6.5021e-03, 4.3516e-03, 5.1580e-03,\n",
            "         7.4297e-03, 4.8862e-03, 1.5733e-03, 2.7631e-03, 2.1936e-03, 4.2592e-03,\n",
            "         3.1519e-03, 9.2840e-01, 3.6775e-03, 2.9041e-03, 4.0387e-03, 6.2410e-03],\n",
            "        [5.9885e-02, 8.7228e-02, 4.3051e-02, 4.3572e-02, 4.2817e-02, 1.0494e-01,\n",
            "         4.4714e-02, 5.1708e-02, 7.0245e-02, 2.5203e-02, 6.1152e-02, 3.9442e-02,\n",
            "         5.6527e-02, 6.3468e-02, 4.2185e-02, 5.6228e-02, 7.4352e-02, 3.3286e-02],\n",
            "        [1.6154e-02, 1.8174e-02, 1.6213e-02, 1.5447e-02, 1.7596e-02, 1.5287e-02,\n",
            "         1.5180e-02, 2.7173e-02, 2.2150e-02, 7.8194e-03, 2.4938e-02, 1.2160e-02,\n",
            "         1.8393e-02, 1.2223e-02, 2.2708e-02, 6.9929e-01, 1.7392e-02, 2.1705e-02],\n",
            "        [1.7372e-02, 1.4899e-02, 5.1443e-03, 1.4792e-02, 1.2977e-02, 1.3766e-02,\n",
            "         1.0566e-02, 1.5124e-02, 1.3800e-02, 4.0607e-03, 1.1025e-02, 1.2344e-02,\n",
            "         9.9543e-03, 1.0800e-02, 1.9645e-02, 1.1365e-02, 7.8813e-01, 1.4241e-02],\n",
            "        [4.9537e-03, 4.2134e-03, 7.1962e-03, 4.4303e-03, 1.0763e-02, 2.4016e-03,\n",
            "         2.5391e-03, 4.6315e-03, 4.2234e-03, 4.6553e-03, 5.6117e-03, 7.4299e-03,\n",
            "         1.9000e-03, 7.1668e-03, 3.2917e-03, 5.0074e-03, 3.3787e-03, 9.1621e-01],\n",
            "        [6.7601e-02, 2.7461e-02, 1.0181e-01, 4.9749e-02, 6.6357e-02, 1.9776e-02,\n",
            "         4.3251e-02, 8.0838e-02, 1.9292e-02, 8.0443e-02, 3.9522e-02, 4.7934e-02,\n",
            "         7.9972e-02, 8.7333e-02, 3.0993e-02, 5.2044e-02, 2.2601e-02, 8.3024e-02]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mu = (1/len(idx))*stochastic_matrix.sum(0)\n",
        "\n",
        "entropy_rate = -(mu*(stochastic_matrix*np.log(stochastic_matrix)).sum(1)).sum(0)/np.log(len(idx))\n",
        "\n",
        "print(\"Entropy rate:\", float(entropy_rate))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7GAb5fCHlpP",
        "outputId": "136d5657-47ce-45a5-d29c-52092526d342"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy rate: 0.4612385034561157\n"
          ]
        }
      ]
    }
  ]
}